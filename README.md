# Самообучение: раскраска изображений (image colorization)
Выполнил Оганов Александр Артурович

Подробный отчет находиться в файле "report.pdf"

## Используемые статьи

Перечислим статьи и кратко опишем статьи, которые использовались в работе
1. [Colorful Image Colorization](https://arxiv.org/abs/1603.08511) - постановка задачи раскраски изображений и сравнение задачи классификации цвета с задачей регрессии (предсказание цвета). Автор расматривает задачу предсказания каналов a, b по каналу L [Lab color space](https://en.wikipedia.org/wiki/CIELAB_color_space), то есть по черно-белому изображению построить цветное;

2. [Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction](https://arxiv.org/abs/1611.09842) - обобщение задачи раскраски изображений на произвольные каналы, в качестве основной модели был взят автокодировщик;

3. [Analysis of Different Losses for Deep Learning Image Colorization](https://arxiv.org/abs/2204.02980) - исследование влияния функции потерь для задачи раскраски изображений, перечеслены основные подходы к решению (классифкация, регрессия, приближение распределений, использование GAN) задачи;

4. [The Unreasonable Effectiveness of Deep Features as a Perceptual Metric](https://arxiv.org/abs/1801.03924) - построение метрики (на основе нейросети), которая отражает похожесть изображений для человека, полученную метрику автор статьи называет lpips.

# Подход к решению поставленной задачи

Решать задачу будем на датасете Food101, выбор обоснован красочностью датасета и его размерами.

В качестве основной модели, следуя статье  [Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction](https://arxiv.org/abs/1611.09842), будем использовать автокодировщик, а в качестве эмбедингов - его латетное пространство. Код для обучения модели представлен в ноутбуке "my colorization.ipynb", для удобства были использованы блоки из библиотеки diffusers, а именно [Encoder и Decoder](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py). Архитектура модели описана в файле "Autoencoder.py".

В качестве функции потерь была взята комбинция двух функций. В след за статьей [Colorful Image Colorization](https://arxiv.org/abs/1603.08511) мы использовали [smooth L1 loss](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html) для попиксельного сравнения и lpips для схожести изображений.

Для оценки полученных представлений проведем следующий эксперимент на датасете Food101. Мы разделим обучающую выборку в соотношние 9:1 для обучения автокодировщика и классификатора соотвественно. Также из обучения автокодировщика исключим 20 классов из 101. Тогда наш эксперемент будет выглядить следующим образом:

Обучение автокодировщика на 81 классе. Обучение классификатора на 1/10  обучащей выборки в латентном пространстве автокодировщика и его сравнение с классификатором, который обучался на 1/10 обучающей выборки


Тем самым мы получим ответы на следующие вопросы:

1. Помогают ли выученные представления в достижении лучшего результата классификации при наличии маленькой обучающей выборки (Эксперимент 1)?

2. Насколько хорошо обобщает данные автокодировщик и влияет ли на результаты классы, которые исполльзовались в его обучении?
