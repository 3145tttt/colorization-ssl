{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c4a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Food101\n",
    "\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "\n",
    "from torchmetrics.image import lpip\n",
    "\n",
    "from diffusers.models.vae import Encoder, Decoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from Autoencoder import Autoencoder\n",
    "from FoodData import FoodColorizationDataset\n",
    "from utils import collect_train_val, get_classes_map, split_in_out_domain, split_train\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_DIR = '../'\n",
    "CHECKPOINT_DIR = './checkpoint_ae'\n",
    "\n",
    "train_dataset = Food101(ROOT_DIR, split='train', transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = Food101(ROOT_DIR, split='test', transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a07eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_id, id_to_class = get_classes_map(ROOT_DIR)\n",
    "classes = np.array(list(class_to_id.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b961401a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['guacamole', 'spring_rolls', 'carrot_cake', 'paella',\n",
       "       'lobster_bisque', 'chicken_wings', 'ravioli', 'sashimi',\n",
       "       'peking_duck', 'peking_duck', 'scallops', 'tuna_tartare',\n",
       "       'churros', 'baklava', 'chocolate_cake', 'gyoza', 'baby_back_ribs',\n",
       "       'scallops', 'cup_cakes', 'filet_mignon'], dtype='<U23')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "out_id = np.random.choice(len(classes), 20)\n",
    "out_classes = classes[out_id]\n",
    "out_classes_id = list([class_to_id[x] for x in out_classes])\n",
    "out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd906c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files, train_target, val_target =\\\n",
    "    collect_train_val(ROOT_DIR)\n",
    "\n",
    "train_in_files, train_in_target, train_out_files, train_out_target =\\\n",
    "    split_in_out_domain(train_files, train_target, out_classes_id)\n",
    "\n",
    "val_in_files, val_in_target, val_out_files, val_out_target =\\\n",
    "    split_in_out_domain(val_files, val_target, out_classes_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7c417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_color_files, train_class_files, train_color_target, train_class_target =\\\n",
    "    split_train(train_in_files, train_in_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed239a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_id, id_to_class = get_classes_map(ROOT_DIR)\n",
    "\n",
    "train_dataset = FoodColorizationDataset(ROOT_DIR, train_color_files, train_color_target, transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Resize((128, 128), antialias=True)\n",
    "        ]), class_to_id, id_to_class, transforms.Compose([\n",
    "            v2.RandomPhotometricDistort(),\n",
    "            transforms.RandomAdjustSharpness(2),\n",
    "            transforms.RandomInvert(),\n",
    "            transforms.Grayscale()\n",
    "        ])\n",
    "        )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=0, shuffle=True)\n",
    "\n",
    "valid_dataset = FoodColorizationDataset(ROOT_DIR, val_in_files, val_in_target, transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Resize((128, 128), antialias=True)\n",
    "        ]), class_to_id, id_to_class, transforms.Compose([\n",
    "            transforms.Grayscale()\n",
    "        ]))\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, num_workers=0, shuffle=False)\n",
    "\n",
    "valid_out_dataset = FoodColorizationDataset(ROOT_DIR, val_out_files, val_out_target, transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Resize((128, 128), antialias=True)\n",
    "        ]), class_to_id, id_to_class, transforms.Compose([\n",
    "            transforms.Grayscale()\n",
    "        ]))\n",
    "\n",
    "valid_out_loader = DataLoader(valid_dataset, batch_size=64, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f01c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "vis_id = np.random.choice(len(train_dataset), 64)\n",
    "vis_train = DataLoader(Subset(train_dataset, vis_id), batch_size=64, shuffle=False)\n",
    "\n",
    "vis_id = np.random.choice(len(valid_dataset), 64)\n",
    "vis_valid = DataLoader(Subset(valid_dataset, vis_id), batch_size=64, shuffle=False)\n",
    "\n",
    "vis_id = np.random.choice(len(valid_out_dataset), 64)\n",
    "vis_out_valid = DataLoader(Subset(valid_out_dataset, vis_id), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f83b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandr/miniconda3/envs/ml/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/alexandr/miniconda3/envs/ml/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "w1 = 1\n",
    "w2 = 1\n",
    "\n",
    "pixel_loss = nn.SmoothL1Loss()\n",
    "perc_loss = lpip.LearnedPerceptualImagePatchSimilarity().cuda()\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    l1 = w1 * pixel_loss(x, y)\n",
    "    l2 = w2 * perc_loss(x, y)\n",
    "    return l1 + l2, l1.item() / w1, l2.item() / w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40acc299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1287631"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=2e-05)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05716201",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation_epoch(model, valid_loader, loss_fn, step):\n",
    "    model.eval()\n",
    "    \n",
    "    loss = 0\n",
    "    l1_total, l2_total = 0, 0\n",
    "    count = 0\n",
    "    for X_batch, target in valid_loader:\n",
    "        X_batch, target = X_batch.cuda(), target.cuda()\n",
    "\n",
    "        out = model(X_batch)\n",
    "        l, l1, l2 = loss_fn(out, target)\n",
    "        loss += l.item() * out.size(0)\n",
    "        l1_total += l1 * out.size(0)\n",
    "        l2_total += l2 * out.size(0)\n",
    "        count += out.size(0)\n",
    "        \n",
    "    loss /= count\n",
    "    l1 /= count\n",
    "    l2 /= count\n",
    "    wandb.log({\"eval/loss\": loss, 'eval/pixel_loss': l1, 'eval/lpips': l2}, step=step)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualization(model, vis_train, vis_valid, vis_out_valid, step):\n",
    "    model.eval()\n",
    "    \n",
    "    def get_img(loader):\n",
    "        for X_batch, target in loader:\n",
    "            X_batch, target = X_batch.cuda(), target.cuda()\n",
    "            out = model(X_batch)\n",
    "            loss, _, _ = loss_fn(out, target)\n",
    "\n",
    "            img = torchvision.utils.make_grid(out, normalize=True).cpu()\n",
    "            return img, loss.item()\n",
    "        \n",
    "    img, loss = get_img(vis_train)\n",
    "    wandb.log({\"vis/train_vis\": wandb.Image(img, caption=f\"mean loss = {loss}\")}, step=step)\n",
    "    \n",
    "    img, loss = get_img(vis_valid)\n",
    "    wandb.log({\"vis/valid_vis\": wandb.Image(img, caption=f\"mean loss = {loss}\")}, step=step)\n",
    "    \n",
    "    img, loss = get_img(vis_out_valid)\n",
    "    wandb.log({\"vis/valid_out_vis\": wandb.Image(img, caption=f\"mean loss = {loss}\")}, step=step)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "def visualization_init(vis_train, vis_valid, vis_out_valid, step):\n",
    "    \n",
    "    for _, target in vis_train:\n",
    "        \n",
    "        img = torchvision.utils.make_grid(target, normalize=True)\n",
    "        wandb.log({\"vis/train\": wandb.Image(img)}, step=step)\n",
    "        \n",
    "    for _, target in vis_valid:\n",
    "        img = torchvision.utils.make_grid(target, normalize=True)\n",
    "        wandb.log({\"vis/valid\": wandb.Image(img)}, step=step)\n",
    "    \n",
    "    for _, target in vis_out_valid:\n",
    "        img = torchvision.utils.make_grid(target, normalize=True)\n",
    "        wandb.log({\"vis/valid_out\": wandb.Image(img)}, step=step)\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, loss_fn,  vis_train, vis_valid,  vis_out_valid, step):\n",
    "    model.train()\n",
    "    for X_batch, target in train_loader:\n",
    "        X_batch, target = X_batch.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X_batch)\n",
    "        loss, l1, l2 = loss_fn(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            visualization(model, vis_train, vis_valid,  vis_out_valid, step)\n",
    "        \n",
    "        wandb.log({\"train/loss\": loss.item(), 'train/pixel_loss': l1, 'train/lpips': l2}, step=step)\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    return step\n",
    "\n",
    "def train(model, optimizer, train_loader, valid_loader, loss_fn, checkpoint_path, \n",
    "            vis_train, vis_valid, vis_out_valid, epoch_num=25):\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    best_loss = 10000\n",
    "    step = 0\n",
    "    \n",
    "    visualization_init(vis_train, vis_valid,  vis_out_valid, step)\n",
    "    for epoch in tqdm(range(epoch_num)):\n",
    "        \n",
    "        step = train_epoch(model, optimizer, train_loader, loss_fn, vis_train, vis_valid, vis_out_valid, step)\n",
    "        loss = validation_epoch(model, valid_loader, loss_fn, step)\n",
    "        \n",
    "        if best_loss > loss:\n",
    "            best_loss = loss\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, checkpoint_path)\n",
    "        step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed818fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"colorization-ae\", name=\"2-ae\")\n",
    "# train(model, optimizer, train_loader, valid_loader, loss_fn,\n",
    "#           checkpoint_path=f\"{CHECKPOINT_DIR}/2ae.pth\",\n",
    "#           vis_train=vis_train, vis_valid=vis_valid, vis_out_valid=vis_out_valid)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef8d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
